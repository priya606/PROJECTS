# -*- coding: utf-8 -*-
"""KNN,Decision Trees(ML-6)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lkeAO8i7E3RSBWMGD1llxpsAubuousZe
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""## KNN algorithm is multi-class classification"""

df=pd.read_csv('/content/drug200.csv')

df.head(10)

df.duplicated().sum()

df.isnull().sum()

print('BP is:',df['BP'].unique())
print('Cholestrol is:',df['Cholesterol'].unique())
print('Drug is:',df['Drug'].unique())

df['Cholesterol']=df['Cholesterol'].map({'HIGH':1,'NORMAL':0}).astype(int)

df['BP']=df['BP'].map({'HIGH':2,'LOW':0,'NORMAL':1}).astype(int)

df['Sex']=df['Sex'].map({'F':1,'M':0}).astype(int)

df.head()

x=df.iloc[:,0:5].values
y=df.iloc[:,-1:].values

x

y

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=5)

from sklearn.neighbors import KNeighborsClassifier
knn_classifier=KNeighborsClassifier(n_neighbors=5)
knn_classifier.fit(xtrain,ytrain)

from sklearn.metrics import accuracy_score
print('accuracy_score for training is:',accuracy_score(ytrain,knn_classifier.predict(xtrain)))
print('accuracy_score for testing is:',accuracy_score(ytest,knn_classifier.predict(xtest)))

""" we can implement multi-class  classification in confusion matrix but it doesnot contains fn,fp along with diagnoal was considered as correct 


"""

from sklearn.metrics import confusion_matrix
cm=confusion_matrix(ytest,knn_classifier.predict(xtest))

import itertools
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=30)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    #print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
            horizontalalignment="center",
            color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

classes=['drugY','drugC','drugX','drugA','drugB']
plt.plot(figsize=(6,6),dpi=30)
plot_confusion_matrix(cm,classes)

acc_train=[]
acc_test=[]
for i in range(1,21):
  knn_classifier=KNeighborsClassifier(n_neighbors=i)
  knn_classifier.fit(xtrain,ytrain)
  acc_train.append(accuracy_score(ytrain,knn_classifier.predict(xtrain)))
  acc_test.append(accuracy_score(ytest,knn_classifier.predict(xtest)))

x=list(range(1,21))
plt.figure(figsize=(7,7),dpi=50)
plt.plot(x,acc_train,color='red',label='training_accuracy')
plt.plot(x,acc_test,color='green',label='testing_accuracy')
plt.legend()
plt.grid()
'''
From graph we can understand that low values in training_accuracy contains high variance in testing_accuracy and high values in training_accuracy contains low variance means high bias
the best fit value in this KNN is where training and testing accuracy coincides i.e,3
'''