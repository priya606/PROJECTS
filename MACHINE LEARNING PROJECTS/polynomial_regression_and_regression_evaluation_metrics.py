# -*- coding: utf-8 -*-
"""Polynomial Regression and  Regression Evaluation Metrics(ML-3)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kUOHC_5mqGbqwjE9iFFyB2JySWJpIGF3
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('/content/Position_Salaries (1).csv - Position_Salaries (1).csv.csv')

df

plt.figure(figsize=(8,8),dpi=70)
plt.plot('Level',data=df,label='level')
plt.plot('Salary',data=df,label='salary')
plt.xlabel('level')
plt.ylabel('salary')
plt.grid()
plt.legend()
plt.xticks(rotation=90)

plt.plot(df['Salary'],df['Level'],color='red')
plt.xlabel('salary')
plt.ylabel('level')

x=df[['Level']].values
y=df[['Salary']].values

x

y

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=50)

from sklearn.linear_model import LinearRegression
lr=LinearRegression()
lr.fit(xtrain,ytrain)

lr.coef_

lr.intercept_

lr.score(x,y)

print(xtrain.shape)
print(xtest.shape)
print(ytrain.shape)
print(ytest.shape)

xtrain

ytrain

xtest

ytest

lr.predict(x)

plt.scatter(x,y,color='red')
plt.plot(x,lr.predict(x),color='green')

'''
here linear function is not capable of capturing non-linear relationship
hence we introduce higher degree variables
'''

from sklearn.preprocessing import PolynomialFeatures
pf=PolynomialFeatures(2)
pf_reg=pf.fit_transform(x)

pf_reg

pf_reg1=LinearRegression()
pf_reg1.fit(pf_reg,y)

pf_reg1.coef_

pf_reg1.intercept_

pf_reg1.score(pf_reg,y)

plt.plot(x,y,color='green')
plt.plot(pf_reg,y,'o',color='red')

plt.plot(x,y,color='green')
plt.plot(x,pf_reg1.predict(pf_reg),'o--',color='red')
#plt.plot(x,lr.predict(x))

from sklearn.preprocessing import PolynomialFeatures
pf=PolynomialFeatures(degree=3)
pf_1=pf.fit_transform(x)

pf_1

poly_regression=LinearRegression()
poly_regression.fit(pf_1,y)

poly_regression.score(pf_1,y)

plt.plot(x,y,color='green')
plt.plot(x,poly_regression.predict(pf_1),'o',color='red')
#plt.plot(x,lr.predict(x))

from sklearn.preprocessing import PolynomialFeatures
pf_5=PolynomialFeatures(degree=5)
pf_6=pf_5.fit_transform(x)
pf_6

pf5_reg=LinearRegression()
pf5_reg.fit(pf_6,y)

pf5_reg.score(pf_6,y)

plt.plot(x,y)
plt.plot(x,pf5_reg.predict(pf_6),'o')

from sklearn.preprocessing import PolynomialFeatures
poly_6=PolynomialFeatures(degree=6)
p6=poly_6.fit_transform(x)

p6

poly_reg6=LinearRegression()
poly_reg6.fit(p6,y)

poly_reg6.score(p6,y)

plt.plot(x,y,'o--',color='green')
plt.plot(x,poly_reg6.predict(p6),'o-',color='red')

plt.plot(x,y,'o',label='actual data')
plt.plot(x,lr.predict(x),label='degree 1')
plt.plot(x,pf_reg1.predict(pf_reg),label='degree 2')
plt.plot(x,poly_regression.predict(pf_1),label='degree 3')
plt.plot(x,pf5_reg.predict(pf_6),label='degree 5')
plt.plot(x,poly_reg6.predict(p6),label='degree 6')
plt.legend()

"""1.over-fitting (high variance) captures unnecessary data in training data and performs poorly in test set and ex.degree(7) in graph
it contains high variance capturing entire training data and poor in testing



2.under-fitting (high bias) performs poorly in both training and testing

3.High bias means cannot able to capture patterns

4.Good balance(required one) means maintaing low variance and low bias

5.overfitting or high variance mainly occurs when complex model is performed on simple data

6.inverse of overfitting(simple models on relatively complex data) is underfitting
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data=pd.read_csv('/content/Position_Salaries (1).csv - Position_Salaries (1).csv.csv')

data.head()

X=sal_data.iloc[:,1:2].values

X

Y=sal_data.iloc[:,-1].values

Y

from sklearn.model_selection import train_test_split
Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,Y,test_size=0.3)

from sklearn.linear_model import LinearRegression
lr=LinearRegression()
lr.fit(Xtrain,Ytrain)

y_pred=lr.predict(Xtest)
y_pred

"""## Mean Absolute Error"""

from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_pred,Ytest)

"""##Mean squared error"""

from sklearn.metrics import mean_squared_error
mean_squared_error(y_pred,Ytest)

"""##Mean root error"""

from sklearn.metrics import mean_squared_error
mean_squared_error(y_pred,Ytest,squared=False)

"""##Mean squared log error"""

from sklearn.metrics import mean_squared_log_error
mean_squared_log_error(y_pred,Ytest)

"""## R-squared score(coefficient of determination)"""

lr.score(Xtest,Ytest)